{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e7131bea",
   "metadata": {},
   "source": [
    "# Packages used\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "545400de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import os.path\n",
    "from tabulate import tabulate\n",
    "from pprint import pprint\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fea6773",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy\n",
    "import scipy\n",
    "import sklearn\n",
    "import openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "465d0f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "#packages used\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pandas as pd\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8f64ddc",
   "metadata": {},
   "source": [
    "# Sentiment Analyisis' training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca366b47",
   "metadata": {},
   "source": [
    "The learning was done on a sample of 200 headlines, each with a \"positive\" or \"negative\" tag based on the news."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7cfcce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#read the training file and clean it\n",
    "df = pd.read_excel(r'C:\\Users\\Utente\\OneDrive\\Desktop\\headlines_folder\\training.xlsx')\n",
    "df = df.replace(\"|\", \"\").replace(\",\",\"\").replace('\"', '').replace(\":\",\"\").replace(\"'\",\"\")\n",
    "df = df.replace(\".\",\"\").replace(\"A\",\"\").replace(\"Ã\",\"\").replace(\"©\",\"\").replace(\"Ã²\",\"\").replace(\"Ã²\",\"\")\n",
    "df =df.replace(\"€™\",\"\")\n",
    "df = df.dropna()\n",
    "\n",
    "#divide the dataset in text and tag\n",
    "x = df[\"text\"] \n",
    "y = df[\"tag\"]\n",
    "\n",
    "#vectorize the text\n",
    "vect = CountVectorizer(ngram_range=(1,1)) \n",
    "X = vect.fit_transform(x)\n",
    "\n",
    "#split the data in training set(80%) and test set(20%)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X , y , test_size=0.2)\n",
    "\n",
    "#Bernoulli model\n",
    "model = BernoulliNB()\n",
    "\n",
    "#Model fitting\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "#results\n",
    "model.predict(X_test)\n",
    "print(model.predict(X_test)) \n",
    "print()\n",
    "accuracy = accuracy_score(y_test, model.predict(X_test))\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b1210ef",
   "metadata": {},
   "source": [
    "# Request newspapers URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81f0bcda",
   "metadata": {},
   "outputs": [],
   "source": [
    "#headlines tgcom\n",
    "url1 = 'https://www.tgcom24.mediaset.it/'\n",
    "response1 = requests.get(url1)\n",
    "soup = bs(response1.text, 'html.parser')\n",
    "headlines1 = soup.find('body').find_all('h2')\n",
    "print()\n",
    "\n",
    "#headline repubblica\n",
    "url2= 'https://www.repubblica.it/'\n",
    "response2 = requests.get(url2)\n",
    "soup = bs(response2.text, 'html.parser')\n",
    "headlines2 = soup.find('body').find_all('h2')\n",
    "\n",
    "#headline fatto quotidiano\n",
    "url3 = \"https://www.ilfattoquotidiano.it/\"\n",
    "response3 = requests.get(url3)\n",
    "soup = bs(response3.text, 'html.parser')\n",
    "headlines3 = soup.find('body').find_all('h2')\n",
    "\n",
    "#headline corriere\n",
    "url4 = \"https://www.corriere.it/\"\n",
    "response4 = requests.get(url4)\n",
    "soup = bs(response4.text, 'html.parser')\n",
    "headlines4 = soup.find('body').find_all(\"h4\")\n",
    "\n",
    "\n",
    "#headline ansa\n",
    "url5 = \"https://www.ansa.it/\"\n",
    "response5 = requests.get(url5)\n",
    "soup = bs(response5.text, 'html.parser')\n",
    "headlines5 = soup.find('body').find_all(\"h3\")\n",
    "\n",
    "total_headlines= [headlines1, headlines2, headlines3, headlines4, headlines5]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ada10b89",
   "metadata": {},
   "source": [
    "# Functions used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16bd4c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#headlines scraper\n",
    "def headlines_scraper():\n",
    "    for x in total_headlines:   #loops that select each newspaper\n",
    "        for y in x:             #loop that read the elements of each newspaper\n",
    "            print(y.text.strip())\n",
    "#headlines scraped in a list form\n",
    "l_raw_text = []\n",
    "for x in total_headlines:   #loops that read the list composed of sublist\n",
    "    for y in x:             #loop that read the elements of the sublist(the headlines)\n",
    "        l_raw_text.append(y.text.strip())\n",
    "#percentage of negative news for sentiment analysis\n",
    "def percentage_of_neg_news(lista):\n",
    "    count = 0\n",
    "    for i in lista:\n",
    "        if i == 'n':\n",
    "            count = count +1\n",
    "    percentage = round((count / len(lista))*100 , 2)\n",
    "    \n",
    "    return percentage \n",
    "\n",
    "#mwnu options\n",
    "def menu_options():\n",
    "    print(\"------------------------------------------------------------------------------------\")\n",
    "    print('\\033[1m' +\"Select a number to perform the desired action:\"+ '\\033[0m')\n",
    "    print(\"1 = Read today's news headlines on the most important italian newspapers\")\n",
    "    print(\"2 = Store today's headlines in the headlines_folder\")\n",
    "    print(\"3 = Read headlines from previous days from headlines_folder\")\n",
    "    print(\"4 = Most used words and word-combinations for a certain day\")\n",
    "    print(\"5 = Sentiment analysis evaluation for a certain day\")\n",
    "    print(\"6 = Terminate\")\n",
    "    print(\"------------------------------------------------------------------------------------\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8f3d8b3",
   "metadata": {},
   "source": [
    "# News Analyzer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a1717f3",
   "metadata": {},
   "source": [
    "**ATTENTION:**   For using the tools it's mandatory to create a folder named \"headlines_folder\" to store the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d622dcce",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### bella\n",
    "print('\\033[1m' +\"NEWS ANALYZER\"+'\\033[0m')\n",
    "print(\"Since ever evil sells and newspapers are prone to negative and catastrophic announcements.\")\n",
    "print(\"Fortunately, there is this tool to monitor this trend!\")\n",
    "print()\n",
    "\n",
    "menu_options()\n",
    "\n",
    "while True:\n",
    "    CHOISE = input('\\033[1m' +\"Select an action: \")\n",
    "    \n",
    "    #Read all the today's news on the most important italian newspapers\n",
    "    if(CHOISE == \"1\"): \n",
    "        print()\n",
    "        print(\"Today's News: \")\n",
    "        print()\n",
    "        headlines_scraper()\n",
    "        menu_options()\n",
    "    #Store the headlines in the headlines_folder\n",
    "    elif(CHOISE == \"2\"):\n",
    "        save_path = r'C:\\Users\\Utente\\OneDrive\\Desktop\\headlines_folder'\n",
    "        file_name = input(\"What is the name of the file: \")\n",
    "        #create the file and write on it today's headlines\n",
    "        completeName = os.path.join(save_path, file_name + \".txt\")\n",
    "        file1 = open(completeName, \"w\", encoding=\"utf-8\")\n",
    "        #first row is text, it will be useful for the machine learnign model\n",
    "        file1.write(\"text \\n\")\n",
    "        #lowercase and write the headlines on the new file\n",
    "        for i in l_raw_text:               \n",
    "            lower_i = str(i.lower())  \n",
    "            file1.write(lower_i + '\\n') \n",
    "        file1.close()\n",
    "        print()\n",
    "        print(\"***file successfully saved*** \")\n",
    "        print()\n",
    "        menu_options()\n",
    "    \n",
    "    elif(CHOISE == \"3\"):\n",
    "        #specify file path and file name\n",
    "        save_path = r'C:\\Users\\Utente\\OneDrive\\Desktop\\headlines_folder'\n",
    "        file_name = input(\"What is the name of the file: \")\n",
    "        #open the file and read headlines \n",
    "        print()\n",
    "        f = open(save_path +\"\\\\\"+ file_name + \".txt\", \"r\", encoding = 'utf-8') \n",
    "        row = f.readline()   # Read the first row of the file\n",
    "        while row :          # while there are rows to read\n",
    "            print(row)\n",
    "            row = f.readline()\n",
    "        f.close() # Mandatory\n",
    "        print()\n",
    "        menu_options()\n",
    "   \n",
    "    elif(CHOISE == \"4\"):\n",
    "        #specify file path and file name\n",
    "        save_path = r'C:\\Users\\Utente\\OneDrive\\Desktop\\headlines_folder'\n",
    "        #open the selected file and read the headlines\n",
    "        selected_headlines = input(\"choose on what headlines data text perform the analysis: \" )\n",
    "        f = open(save_path +\"\\\\\"+ selected_headlines + \".txt\", \"r\", encoding = 'utf-8')\n",
    "        s_raw_text = f.read() \n",
    "        #clean the text from symbols\n",
    "        s_raw_text_nosymb = s_raw_text.replace(\"|\", \"\").replace(\",\",\"\").replace('\"', '').replace(\":\",\"\").replace(\"'\",\"\").replace(\".\",\"\") #pulisci i titoli dai simboli\n",
    "        #transform the text into a list to take out the stopwords\n",
    "        l_raw_text_nosymb: list[str] = nltk.word_tokenize(s_raw_text_nosymb) #transform s_raw_text_nosymb into a list\n",
    "        \n",
    "        stopwords = [\"il\",\"lo\",\"la\",\"i\",\"gli\",\"le\",\"di\",\"a\",\"da\", \"in\",\"con\",\"su\",\"per\", \"tra\",\\\n",
    "             \"fra\",\"e\",\"è\",\"nel\",\"dei\",\"delle\",\"degli\",\"dello\",\"del\",\"dal\",\"al\"\\\n",
    "            ,'del','un','«','»','della','’','che','una','più','?','ecco','alla','(',')','si','l','dalla','come','ha','ma',\\\n",
    "            'ai','foto', 'nella','-', 'anche','sono','“', '”''sua', 'ci','ã¨', 'sulla', 'alle', 'de', 'ad', 'mi','sui',\\\n",
    "             \"sul\",\"prima\",'sotto','sempre','fa','fino', 'ho','suo',\"chi\",\"non\",'”','video',\"mio\",'dai',\"così\",\"uno\",\\\n",
    "            \"due\",\"tre\",\"ora\",\"dopo\",\"perché\",\"%\"]\n",
    "       \n",
    "        #loop for creating the clean_text list without stopwords\n",
    "        clean_text = []\n",
    "        for word in l_raw_text_nosymb :\n",
    "            if word not in (stopwords):\n",
    "                clean_text.append(word)\n",
    "       \n",
    "        #print in a table the 10 words with higest frequency\n",
    "        \n",
    "        fd = nltk.FreqDist(clean_text)\n",
    "        print(tabulate(fd.most_common(10),headers = [\"Most Common words\", \"frequency\"],tablefmt=\"pretty\"))\n",
    "       \n",
    "        #print in a table the 3 most common words' trio\n",
    "        \n",
    "        finder = nltk.collocations.TrigramCollocationFinder.from_words(clean_text)\n",
    "        print(tabulate(finder.ngram_fd.most_common(3), headers=[\"Most Commons Words Trio \",\"frequency\"],tablefmt=\"pretty\"))\n",
    "        print()\n",
    "        menu_options()\n",
    "        #aggiunta\n",
    "        \n",
    "    elif(CHOISE == \"5\"):\n",
    "        save_path = r'C:\\Users\\Utente\\OneDrive\\Desktop\\headlines_folder'\n",
    "        #open the selected file and read the headlines\n",
    "\n",
    "        selected_headlines = input(\"choose on which file perform the sentiment analysis: \" )\n",
    "        #if selected file exist in excel type than perform the sentiment analysis directly\n",
    "        if os.path.exists(save_path +\"\\\\\"+ selected_headlines) == True:\n",
    "\n",
    "\n",
    "            #quick text cleaning\n",
    "            dfp = pd.read_excel(save_path +\"\\\\\"+ selected_headlines +'.xlsx')\n",
    "            dfp.replace(\"|\", \"\").replace(\",\",\"\").replace('\"', '').replace(\":\",\"\").replace(\"'\",\"\").replace(\".\",\"\").replace(\"A\",\"\").replace(\"Ã\",\"\").replace(\"©\",\"\")\n",
    "            dfp = dfp.dropna()\n",
    "            #perform the sentiment analysis\n",
    "            x = dfp['text ']\n",
    "            X = vect.transform(x)\n",
    "\n",
    "            model_trained = pickle.load(open('model.pkl', 'rb'))\n",
    "            vect = pickle.load(open('tfidf.pkl', 'rb'))\n",
    "            X = vect.transform(x)\n",
    "\n",
    "            model_trained.predict(X)\n",
    "\n",
    "            print(model_trained.predict(X))\n",
    "            #mshow negative news percentage\n",
    "\n",
    "            print()\n",
    "            print(\"The estimated percentage of negative news is: \", percentage)\n",
    "\n",
    "        #if the file does not exist in excel format create it\n",
    "        else:\n",
    "            #create theexcel file\n",
    "            read_file = pd.read_csv(save_path +\"\\\\\"+ selected_headlines + \".txt\", on_bad_lines='skip')\n",
    "            file_excel = read_file.to_excel(save_path +\"\\\\\"+ selected_headlines +'.xlsx', index = None, header=True)\n",
    "\n",
    "            dfp = pd.read_excel(save_path +\"\\\\\"+ selected_headlines +'.xlsx')\n",
    "            dfp.replace(\"|\", \"\").replace(\",\",\"\").replace('\"', '').replace(\":\",\"\").replace(\"'\",\"\").replace(\".\",\"\").replace(\"A\",\"\").replace(\"Ã\",\"\").replace(\"©\",\"\")\n",
    "            dfp = dfp.dropna()\n",
    "            #perform the sentiment analysis\n",
    "            x = dfp['text ']\n",
    "            X = vect.transform(x)\n",
    "\n",
    "            model_trained = pickle.load(open('model.pkl', 'rb'))\n",
    "            vect = pickle.load(open('tfidf.pkl', 'rb'))\n",
    "            X = vect.transform(x)\n",
    "\n",
    "            result = model_trained.predict(X)\n",
    "\n",
    "            print()\n",
    "            print('\\033[1m' +\"The estimated negative news percentage is: \"+'\\033[0m', percentage_of_neg_news(result))\n",
    "            print()\n",
    "            menu_options()\n",
    "    \n",
    "    elif(CHOISE == \"6\"):\n",
    "        break\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17618876",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f39f8a3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
